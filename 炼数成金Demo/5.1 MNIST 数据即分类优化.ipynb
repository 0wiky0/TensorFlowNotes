{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter 0, Texting acc 0.9501, train_acc acc 0.95378184\n",
      "Iter 1, Texting acc 0.962, train_acc acc 0.9687273\n",
      "Iter 2, Texting acc 0.966, train_acc acc 0.9752182\n",
      "Iter 3, Texting acc 0.9689, train_acc acc 0.981\n",
      "Iter 4, Texting acc 0.9705, train_acc acc 0.98332727\n",
      "Iter 5, Texting acc 0.9737, train_acc acc 0.9882909\n",
      "Iter 6, Texting acc 0.9747, train_acc acc 0.9898\n",
      "Iter 7, Texting acc 0.9768, train_acc acc 0.99076366\n",
      "Iter 8, Texting acc 0.9787, train_acc acc 0.9926\n",
      "Iter 9, Texting acc 0.9781, train_acc acc 0.9936182\n",
      "Iter 10, Texting acc 0.9794, train_acc acc 0.9938909\n",
      "Iter 11, Texting acc 0.979, train_acc acc 0.9942909\n",
      "Iter 12, Texting acc 0.9794, train_acc acc 0.9946909\n",
      "Iter 13, Texting acc 0.9785, train_acc acc 0.9946182\n",
      "Iter 14, Texting acc 0.9805, train_acc acc 0.99505454\n",
      "Iter 15, Texting acc 0.9815, train_acc acc 0.9954909\n",
      "Iter 16, Texting acc 0.9818, train_acc acc 0.9957273\n",
      "Iter 17, Texting acc 0.9812, train_acc acc 0.99596363\n",
      "Iter 18, Texting acc 0.9802, train_acc acc 0.9957273\n",
      "Iter 19, Texting acc 0.9821, train_acc acc 0.9964182\n",
      "Iter 20, Texting acc 0.9829, train_acc acc 0.9965818\n",
      "Iter 21, Texting acc 0.9826, train_acc acc 0.99667275\n",
      "Iter 22, Texting acc 0.9819, train_acc acc 0.9961636\n",
      "Iter 23, Texting acc 0.9783, train_acc acc 0.99605453\n",
      "Iter 24, Texting acc 0.9818, train_acc acc 0.99685454\n",
      "Iter 25, Texting acc 0.9804, train_acc acc 0.99694544\n",
      "Iter 26, Texting acc 0.9826, train_acc acc 0.9970545\n",
      "Iter 27, Texting acc 0.9805, train_acc acc 0.99701816\n",
      "Iter 28, Texting acc 0.9824, train_acc acc 0.9972182\n",
      "Iter 29, Texting acc 0.9821, train_acc acc 0.9972\n",
      "Iter 30, Texting acc 0.9818, train_acc acc 0.9972364\n",
      "Iter 31, Texting acc 0.9819, train_acc acc 0.99741817\n",
      "Iter 32, Texting acc 0.982, train_acc acc 0.99750906\n",
      "Iter 33, Texting acc 0.9812, train_acc acc 0.9975455\n",
      "Iter 34, Texting acc 0.9813, train_acc acc 0.9975455\n",
      "Iter 35, Texting acc 0.9818, train_acc acc 0.99756366\n",
      "Iter 36, Texting acc 0.9816, train_acc acc 0.9974727\n",
      "Iter 37, Texting acc 0.9814, train_acc acc 0.9976182\n",
      "Iter 38, Texting acc 0.9823, train_acc acc 0.99765456\n",
      "Iter 39, Texting acc 0.9825, train_acc acc 0.99767274\n",
      "Iter 40, Texting acc 0.9829, train_acc acc 0.9976909\n",
      "Iter 41, Texting acc 0.9828, train_acc acc 0.9977091\n",
      "Iter 42, Texting acc 0.9824, train_acc acc 0.9977273\n",
      "Iter 43, Texting acc 0.9822, train_acc acc 0.9977273\n",
      "Iter 44, Texting acc 0.9831, train_acc acc 0.99774545\n",
      "Iter 45, Texting acc 0.9833, train_acc acc 0.99776363\n",
      "Iter 46, Texting acc 0.9833, train_acc acc 0.99776363\n",
      "Iter 47, Texting acc 0.9823, train_acc acc 0.9978\n",
      "Iter 48, Texting acc 0.9835, train_acc acc 0.9978182\n",
      "Iter 49, Texting acc 0.9828, train_acc acc 0.99783635\n",
      "Iter 50, Texting acc 0.9833, train_acc acc 0.9978727\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "\n",
    "# 定义每个批次的大小\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# # 根据样本样式定义创建占位符（1列）\n",
    "x = tf.placeholder(tf.float32,[None,784]) # 每张图的像素点信息： 28*28 = 784\n",
    "y = tf.placeholder(tf.float32,[None,10])  # 十个数字\n",
    "keep_prob = tf.placeholder(tf.float32)  \n",
    "lr =  tf.Variable(0.001, dtype = tf.float32) # 学习率变量\n",
    "\n",
    "# 定义神经网络的中间层\n",
    "# \n",
    "# （输入层784个数据，中间层500个神经元）\n",
    "W1 = tf.Variable(tf.truncated_normal([784,500],stddev=0.1)) \n",
    "b1 =  tf.Variable(tf.zeros([500])+0.1)\n",
    "# 激活函数：tanh\n",
    "L1 = tf.nn.tanh(tf.matmul(x,W1) + b1)\n",
    "L1_drop =  tf.nn.dropout(L1,keep_prob)\n",
    "\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([500,300],stddev=0.1)) \n",
    "b2 =  tf.Variable(tf.zeros([300])+0.1)\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_drop,W2) + b2)\n",
    "L2_drop =  tf.nn.dropout(L2,keep_prob)\n",
    "\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([300,10],stddev=0.1)) \n",
    "b4 =  tf.Variable(tf.zeros([10])+0.1)\n",
    "L4 = tf.matmul(L2_drop,W4) + b4\n",
    "prediction = tf.nn.softmax(L4)\n",
    "\n",
    "\n",
    "# 二次代阶函数\n",
    "# loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "\n",
    "# 交叉熵\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y, logits = prediction))\n",
    "\n",
    "# 定义一个梯度下降法来进行训练的优化器\n",
    "train =  tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "# 结果存放在一个布尔型的列表中\n",
    "correct_prediction = tf.equal(tf.arg_max(y,1),tf.arg_max(prediction,1))# argmax 返回一维张量中最大值的所以在位置\n",
    "# 求准确率\n",
    "# cast 把布尔型列表转换为float32， 如[true.true.false] =》 [1,1,0] ，那么准确率的值即为66.6%\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    # 初始化全部变量\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 训练21个周期\n",
    "    for epoch in range(51):\n",
    "        # 每个周期都改变学习率（学习越久学习率越低）\n",
    "        sess.run(tf.assign(lr, 0.001*(0.95**epoch)))\n",
    "        # n_batch ： 一共有多少个批次\n",
    "        for batch in range(n_batch):\n",
    "            # 保存batch_size张图片的数据与标签\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0})\n",
    "        \n",
    "        # 用测试集的图片及标签求得准确率\n",
    "        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "        train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0})\n",
    "        print(\"Iter \"+ str(epoch) + \", Texting acc \" + str(test_acc)+ \", train_acc acc \" + str(train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
